# Model Evaluation and Selection

| Topic | Description |
|-------|------------|
| [Bias-Variance Tradeoff](./bias-variance.md) | Understanding how bias and variance affect the performance of models. |
| [Cross Validation](./cross-validation.md) | Techniques to validate a modelâ€™s performance. |
| [Loss Functions](./loss-function.md) | An introduction to loss functions and their role in training models. |
| [Overfitting](./overfitting.md) | How to detect and mitigate overfitting in models. |
| [Underfitting](./underfitting.md) | Understanding underfitting and its impact on model performance. |

> ### Metrics for Classification Models

| Metric | Description |
|--------|------------|
| [Accuracy Score](./for-classification/accuracy-score.md) | The overall accuracy of classification models. |
| [Confusion Matrix](./for-classification/confusion-matrix.md) | A detailed matrix showing model predictions vs actual outcomes. |
| [Recall Score](./for-classification/recall-score.md) | A measure of the true positive rate in classification tasks. |
| [Precision Score](./for-classification/precision-score.md) | The precision of classification, measuring relevant predictions. |
| [F1 Score](./for-classification/f1-score.md) | A combined measure of precision and recall. |

> ### Metrics for Regression Models

| Metric | Description |
|--------|------------|
| [Mean Absolute Error](./for-regression/Mean-Absolute-Error.md) | The mean of absolute errors between predictions and actual values. |
