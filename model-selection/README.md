# Model Evaluation and Selection

| Topic | Description |
|-------|------------|
| [Bias-Variance Tradeoff](./bias-variance.md) | Understanding how bias and variance affect the performance of models. |
| [Cross Validation](./cross-validation.md) | Techniques to validate a modelâ€™s performance. |
| [Loss Functions](./loss-function.md) | An introduction to loss functions and their role in training models. |
| [Overfitting](./overfitting.md) | How to detect and mitigate overfitting in models. |
| [Underfitting](./underfitting.md) | Understanding underfitting and its impact on model performance. |

> ## Metrics for Classification Models

| Metric | Description |
|--------|------------|
| [Accuracy Score](./classification/accuracy-score.md) | The overall accuracy of classification models. |
| [Confusion Matrix](./classification/confusion-matrix.md) | A detailed matrix showing model predictions vs actual outcomes. |
| [Recall Score](./classification/recall-score.md) | A measure of the true positive rate in classification tasks. |
| [Precision Score](./classification/precision-score.md) | The precision of classification, measuring relevant predictions. |
| [F1 Score](./classification/f1-score.md) | A combined measure of precision and recall. |

[Implementation of Classification Models Evaluation Metrics From Scratch Using Python](./classification/_implementation.ipynb)

> ## Metrics for Regression Models

| Metric | Description |
|--------|------------|
| [Mean Squared Error](./regression/MSE.md) | The mean of squared errors between predictions and actual values. |
| [Mean Absolute Error](./regression/MAE.md) | The mean of absolute errors between predictions and actual values. |
